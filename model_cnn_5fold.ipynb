{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dYxpVTQDDex5",
    "outputId": "4754ea58-e3f9-402b-f93f-dcdb4dbb0939"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import Model\n",
    "from keras.layers import Embedding, Input, Reshape, Flatten, Concatenate, Conv2D, MaxPool2D, Dense, Dropout, SpatialDropout1D\n",
    "from keras.layers import BatchNormalization, Dropout, Activation\n",
    "from keras.layers import GlobalMaxPool1D, GlobalAveragePooling1D, GlobalAvgPool1D, GlobalMaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback\n",
    "from keras.utils import to_categorical\n",
    "from keras_radam import RAdam\n",
    "from keras_lookahead import Lookahead\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "KTF.set_session(session)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keras-rectified-adam\n",
    "# !pip install keras-lookahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    tf.set_random_seed(seed)\n",
    "\n",
    "seed = 2020\n",
    "fix_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c7LMAeCXD3Tr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('raw_data/train_set.csv', sep='\\t')\n",
    "df_test = pd.read_csv('raw_data/test_a.csv', sep='\\t')\n",
    "df_data = df_train.append(df_test)\n",
    "df_data = df_data.reset_index(drop=True)\n",
    "df_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2967 6758 339 2021 1854 3731 4109 3792 4149 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.0</td>\n",
       "      <td>4464 486 6352 5619 2465 4802 1452 3137 5778 54...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>7346 4068 5074 3747 5681 6093 1777 2226 7354 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>7159 948 4866 2109 5520 2490 211 3956 5520 549...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3646 3055 3055 2490 4659 6065 3370 5814 2465 5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0    2.0  2967 6758 339 2021 1854 3731 4109 3792 4149 15...\n",
       "1   11.0  4464 486 6352 5619 2465 4802 1452 3137 5778 54...\n",
       "2    3.0  7346 4068 5074 3747 5681 6093 1777 2226 7354 6...\n",
       "3    2.0  7159 948 4866 2109 5520 2490 211 3956 5520 549...\n",
       "4    3.0  3646 3055 3055 2490 4659 6065 3370 5814 2465 5..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pwyczuurF9zl",
    "outputId": "2c9cff1e-5961-4d5f-8a64-582ac0e3e6db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate seqs\n"
     ]
    }
   ],
   "source": [
    "max_words_num = None\n",
    "seq_len = 2000\n",
    "embedding_dim = 128\n",
    "col = 'text'\n",
    "\n",
    "print('Generate seqs')\n",
    "os.makedirs('seqs', exist_ok=True)\n",
    "seq_path = 'seqs/seqs_{}_{}.npy'.format(max_words_num, seq_len)\n",
    "word_index_path = 'seqs/word_index_{}_{}.npy'.format(max_words_num, seq_len)\n",
    "if not os.path.exists(seq_path) or not os.path.exists(word_index_path):\n",
    "    tokenizer = text.Tokenizer(num_words=max_words_num, lower=False, filters='')\n",
    "    tokenizer.fit_on_texts(df_data[col].values.tolist())\n",
    "    seqs = sequence.pad_sequences(tokenizer.texts_to_sequences(df_data[col].values.tolist()), maxlen=seq_len,\n",
    "                                  padding='post', truncating='pre')\n",
    "    word_index = tokenizer.word_index\n",
    "        \n",
    "    np.save(seq_path, seqs)\n",
    "    np.save(word_index_path, word_index)\n",
    "\n",
    "else:\n",
    "    seqs = np.load(seq_path)\n",
    "    word_index = np.load(word_index_path, allow_pickle=True).item()\n",
    "    \n",
    "# print('Generate embedding')\n",
    "# os.makedirs('embedding', exist_ok=True)\n",
    "# embedding_path = 'embedding/w2v_{}_{}.m'.format(col, embedding_dim)\n",
    "# if not os.path.exists(embedding_path):\n",
    "#     print('Training w2v')\n",
    "#     model = Word2Vec([[word for word in senetnce.split(' ')] for senetnce in df_data[col].values],\n",
    "#                       size=embedding_dim, window=20, workers=32, seed=seed, min_count=1, sg=1, hs=1)\n",
    "\n",
    "#     model.save(embedding_path)\n",
    "# else:\n",
    "#     model = Word2Vec.load(embedding_path)\n",
    "\n",
    "embedding = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "# for word, i in tqdm(word_index.items()):\n",
    "#     embedding_vector = model[word] if word in model else None\n",
    "#     if embedding_vector is not None:\n",
    "#         embedding[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0     38918\n",
       "1.0     36945\n",
       "2.0     31425\n",
       "3.0     22133\n",
       "4.0     15016\n",
       "5.0     12232\n",
       "6.0      9985\n",
       "7.0      8841\n",
       "8.0      7847\n",
       "9.0      5878\n",
       "10.0     4920\n",
       "11.0     3131\n",
       "12.0     1821\n",
       "13.0      908\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('model', exist_ok=True)\n",
    "os.makedirs('sub', exist_ok=True)\n",
    "os.makedirs('prob', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_index = df_data[df_data['label'].notnull()].index.tolist()\n",
    "test_index = df_data[df_data['label'].isnull()].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(emb, seq_len, filter_sizes, num_filters):\n",
    "    embed_size = emb.shape[1]\n",
    "    \n",
    "    emb_layer = Embedding(\n",
    "        input_dim=emb.shape[0],\n",
    "        output_dim=emb.shape[1],\n",
    "#         weights=[emb],\n",
    "        input_length=seq_len,\n",
    "#         trainable=False\n",
    "    )\n",
    "    \n",
    "    seq = Input(shape=(seq_len, ))\n",
    "    seq_emb = emb_layer(seq)\n",
    "    seq_emb = SpatialDropout1D(rate=0.2)(seq_emb)\n",
    "\n",
    "    x = Reshape((seq_len, embed_size, 1))(seq_emb)\n",
    "\n",
    "    maxpool_pool = []\n",
    "    for i in range(len(filter_sizes)):\n",
    "        conv = Conv2D(num_filters, kernel_size=(filter_sizes[i], embed_size), kernel_initializer='he_normal', activation='relu')(x)\n",
    "        maxpool_pool.append(MaxPool2D(pool_size=(seq_len - filter_sizes[i] + 1, 1))(conv))        \n",
    "    \n",
    "    x = Concatenate(axis=1)(maxpool_pool)   \n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    x = Dropout(0.2)(Activation(activation='relu')(BatchNormalization()(Dense(1024)(x))))\n",
    "    out = Dense(14, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=seq, outputs=out)\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=Lookahead(RAdam()), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator(Callback):\n",
    "    def __init__(self, validation_data):\n",
    "        super().__init__()\n",
    "        self.best_val_f1 = 0.\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "\n",
    "    def evaluate(self):\n",
    "        y_true = self.y_val\n",
    "        y_pred = self.model.predict(self.x_val).argmax(axis=1)\n",
    "        f1 = f1_score(y_true, y_pred, average='macro')\n",
    "        return f1\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_f1 = self.evaluate()\n",
    "        if val_f1 > self.best_val_f1:\n",
    "            self.best_val_f1 = val_f1\n",
    "        logs['val_f1'] = val_f1\n",
    "        print(f'val_f1: {val_f1:.5f}, best_val_f1: {self.best_val_f1:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 160000 samples, validate on 40000 samples\n",
      "Epoch 1/30\n",
      "160000/160000 [==============================] - 341s 2ms/step - loss: 0.7733 - accuracy: 0.7672 - val_loss: 0.3037 - val_accuracy: 0.9079\n",
      "val_f1: 0.88343, best_val_f1: 0.88343\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.88343, saving model to model/cnn_0.h5\n",
      "Epoch 2/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.2180 - accuracy: 0.9322 - val_loss: 0.5842 - val_accuracy: 0.8359\n",
      "val_f1: 0.75280, best_val_f1: 0.88343\n",
      "\n",
      "Epoch 00002: val_f1 did not improve from 0.88343\n",
      "Epoch 3/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.1548 - accuracy: 0.9509 - val_loss: 0.2374 - val_accuracy: 0.9290\n",
      "val_f1: 0.91287, best_val_f1: 0.91287\n",
      "\n",
      "Epoch 00003: val_f1 improved from 0.88343 to 0.91287, saving model to model/cnn_0.h5\n",
      "Epoch 4/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.1149 - accuracy: 0.9625 - val_loss: 0.8120 - val_accuracy: 0.8131\n",
      "val_f1: 0.79302, best_val_f1: 0.91287\n",
      "\n",
      "Epoch 00004: val_f1 did not improve from 0.91287\n",
      "Epoch 5/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.0856 - accuracy: 0.9716 - val_loss: 0.3623 - val_accuracy: 0.9117\n",
      "val_f1: 0.89945, best_val_f1: 0.91287\n",
      "\n",
      "Epoch 00005: val_f1 did not improve from 0.91287\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 6/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.0517 - accuracy: 0.9829 - val_loss: 0.2783 - val_accuracy: 0.9319\n",
      "val_f1: 0.92534, best_val_f1: 0.92534\n",
      "\n",
      "Epoch 00006: val_f1 improved from 0.91287 to 0.92534, saving model to model/cnn_0.h5\n",
      "Epoch 7/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.0364 - accuracy: 0.9878 - val_loss: 0.2510 - val_accuracy: 0.9389\n",
      "val_f1: 0.92940, best_val_f1: 0.92940\n",
      "\n",
      "Epoch 00007: val_f1 improved from 0.92534 to 0.92940, saving model to model/cnn_0.h5\n",
      "Epoch 8/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.0301 - accuracy: 0.9900 - val_loss: 0.3093 - val_accuracy: 0.9323\n",
      "val_f1: 0.91867, best_val_f1: 0.92940\n",
      "\n",
      "Epoch 00008: val_f1 did not improve from 0.92940\n",
      "Epoch 9/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.0257 - accuracy: 0.9915 - val_loss: 0.2912 - val_accuracy: 0.9349\n",
      "val_f1: 0.92350, best_val_f1: 0.92940\n",
      "\n",
      "Epoch 00009: val_f1 did not improve from 0.92940\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 10/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.0186 - accuracy: 0.9939 - val_loss: 0.2612 - val_accuracy: 0.9460\n",
      "val_f1: 0.93771, best_val_f1: 0.93771\n",
      "\n",
      "Epoch 00010: val_f1 improved from 0.92940 to 0.93771, saving model to model/cnn_0.h5\n",
      "Epoch 11/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0147 - accuracy: 0.9951 - val_loss: 0.2659 - val_accuracy: 0.9463\n",
      "val_f1: 0.93850, best_val_f1: 0.93850\n",
      "\n",
      "Epoch 00011: val_f1 improved from 0.93771 to 0.93850, saving model to model/cnn_0.h5\n",
      "Epoch 12/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.0123 - accuracy: 0.9960 - val_loss: 0.2720 - val_accuracy: 0.9467\n",
      "val_f1: 0.93907, best_val_f1: 0.93907\n",
      "\n",
      "Epoch 00012: val_f1 improved from 0.93850 to 0.93907, saving model to model/cnn_0.h5\n",
      "Epoch 13/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.0122 - accuracy: 0.9963 - val_loss: 0.2955 - val_accuracy: 0.9436\n",
      "val_f1: 0.93648, best_val_f1: 0.93907\n",
      "\n",
      "Epoch 00013: val_f1 did not improve from 0.93907\n",
      "Epoch 14/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.0115 - accuracy: 0.9963 - val_loss: 0.2882 - val_accuracy: 0.9458\n",
      "val_f1: 0.93897, best_val_f1: 0.93907\n",
      "\n",
      "Epoch 00014: val_f1 did not improve from 0.93907\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 15/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0093 - accuracy: 0.9972 - val_loss: 0.2961 - val_accuracy: 0.9462\n",
      "val_f1: 0.93968, best_val_f1: 0.93968\n",
      "\n",
      "Epoch 00015: val_f1 improved from 0.93907 to 0.93968, saving model to model/cnn_0.h5\n",
      "Epoch 16/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0082 - accuracy: 0.9975 - val_loss: 0.2905 - val_accuracy: 0.9463\n",
      "val_f1: 0.93969, best_val_f1: 0.93969\n",
      "\n",
      "Epoch 00016: val_f1 improved from 0.93968 to 0.93969, saving model to model/cnn_0.h5\n",
      "Epoch 17/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0074 - accuracy: 0.9978 - val_loss: 0.2931 - val_accuracy: 0.9470\n",
      "val_f1: 0.93940, best_val_f1: 0.93969\n",
      "\n",
      "Epoch 00017: val_f1 did not improve from 0.93969\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 18/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0070 - accuracy: 0.9979 - val_loss: 0.2902 - val_accuracy: 0.9482\n",
      "val_f1: 0.94177, best_val_f1: 0.94177\n",
      "\n",
      "Epoch 00018: val_f1 improved from 0.93969 to 0.94177, saving model to model/cnn_0.h5\n",
      "Epoch 19/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0066 - accuracy: 0.9979 - val_loss: 0.2913 - val_accuracy: 0.9484\n",
      "val_f1: 0.94163, best_val_f1: 0.94177\n",
      "\n",
      "Epoch 00019: val_f1 did not improve from 0.94177\n",
      "Epoch 20/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0063 - accuracy: 0.9980 - val_loss: 0.2898 - val_accuracy: 0.9491\n",
      "val_f1: 0.94231, best_val_f1: 0.94231\n",
      "\n",
      "Epoch 00020: val_f1 improved from 0.94177 to 0.94231, saving model to model/cnn_0.h5\n",
      "Epoch 21/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0056 - accuracy: 0.9984 - val_loss: 0.3009 - val_accuracy: 0.9475\n",
      "val_f1: 0.94092, best_val_f1: 0.94231\n",
      "\n",
      "Epoch 00021: val_f1 did not improve from 0.94231\n",
      "Epoch 22/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0055 - accuracy: 0.9984 - val_loss: 0.3009 - val_accuracy: 0.9469\n",
      "val_f1: 0.94046, best_val_f1: 0.94231\n",
      "\n",
      "Epoch 00022: val_f1 did not improve from 0.94231\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 23/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0054 - accuracy: 0.9985 - val_loss: 0.2936 - val_accuracy: 0.9496\n",
      "val_f1: 0.94309, best_val_f1: 0.94309\n",
      "\n",
      "Epoch 00023: val_f1 improved from 0.94231 to 0.94309, saving model to model/cnn_0.h5\n",
      "Epoch 24/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0049 - accuracy: 0.9986 - val_loss: 0.2957 - val_accuracy: 0.9487\n",
      "val_f1: 0.94222, best_val_f1: 0.94309\n",
      "\n",
      "Epoch 00024: val_f1 did not improve from 0.94309\n",
      "Epoch 25/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0050 - accuracy: 0.9986 - val_loss: 0.3013 - val_accuracy: 0.9479\n",
      "val_f1: 0.94140, best_val_f1: 0.94309\n",
      "\n",
      "Epoch 00025: val_f1 did not improve from 0.94309\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 26/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0045 - accuracy: 0.9988 - val_loss: 0.2940 - val_accuracy: 0.9490\n",
      "val_f1: 0.94247, best_val_f1: 0.94309\n",
      "\n",
      "Epoch 00026: val_f1 did not improve from 0.94309\n",
      "Epoch 27/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0046 - accuracy: 0.9988 - val_loss: 0.2945 - val_accuracy: 0.9492\n",
      "val_f1: 0.94241, best_val_f1: 0.94309\n",
      "\n",
      "Epoch 00027: val_f1 did not improve from 0.94309\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 28/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0045 - accuracy: 0.9988 - val_loss: 0.2951 - val_accuracy: 0.9490\n",
      "val_f1: 0.94244, best_val_f1: 0.94309\n",
      "\n",
      "Epoch 00028: val_f1 did not improve from 0.94309\n",
      "Epoch 00028: early stopping\n",
      "Train on 160000 samples, validate on 40000 samples\n",
      "Epoch 1/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.7958 - accuracy: 0.7586 - val_loss: 0.3272 - val_accuracy: 0.9002\n",
      "val_f1: 0.86945, best_val_f1: 0.86945\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.86945, saving model to model/cnn_1.h5\n",
      "Epoch 2/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.2210 - accuracy: 0.9310 - val_loss: 0.2316 - val_accuracy: 0.9304\n",
      "val_f1: 0.90903, best_val_f1: 0.90903\n",
      "\n",
      "Epoch 00002: val_f1 improved from 0.86945 to 0.90903, saving model to model/cnn_1.h5\n",
      "Epoch 3/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.1561 - accuracy: 0.9500 - val_loss: 0.3152 - val_accuracy: 0.9100\n",
      "val_f1: 0.89922, best_val_f1: 0.90903\n",
      "\n",
      "Epoch 00003: val_f1 did not improve from 0.90903\n",
      "Epoch 4/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.1163 - accuracy: 0.9620 - val_loss: 0.3848 - val_accuracy: 0.9018\n",
      "val_f1: 0.87423, best_val_f1: 0.90903\n",
      "\n",
      "Epoch 00004: val_f1 did not improve from 0.90903\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 5/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0713 - accuracy: 0.9765 - val_loss: 0.2154 - val_accuracy: 0.9419\n",
      "val_f1: 0.93108, best_val_f1: 0.93108\n",
      "\n",
      "Epoch 00005: val_f1 improved from 0.90903 to 0.93108, saving model to model/cnn_1.h5\n",
      "Epoch 6/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0529 - accuracy: 0.9826 - val_loss: 0.3154 - val_accuracy: 0.9258\n",
      "val_f1: 0.91713, best_val_f1: 0.93108\n",
      "\n",
      "Epoch 00006: val_f1 did not improve from 0.93108\n",
      "Epoch 7/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0420 - accuracy: 0.9858 - val_loss: 0.3084 - val_accuracy: 0.9316\n",
      "val_f1: 0.91867, best_val_f1: 0.93108\n",
      "\n",
      "Epoch 00007: val_f1 did not improve from 0.93108\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 8/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0290 - accuracy: 0.9907 - val_loss: 0.2626 - val_accuracy: 0.9395\n",
      "val_f1: 0.93429, best_val_f1: 0.93429\n",
      "\n",
      "Epoch 00008: val_f1 improved from 0.93108 to 0.93429, saving model to model/cnn_1.h5\n",
      "Epoch 9/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0232 - accuracy: 0.9927 - val_loss: 0.2675 - val_accuracy: 0.9427\n",
      "val_f1: 0.93385, best_val_f1: 0.93429\n",
      "\n",
      "Epoch 00009: val_f1 did not improve from 0.93429\n",
      "Epoch 10/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0191 - accuracy: 0.9939 - val_loss: 0.2756 - val_accuracy: 0.9431\n",
      "val_f1: 0.93401, best_val_f1: 0.93429\n",
      "\n",
      "Epoch 00010: val_f1 did not improve from 0.93429\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 11/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0157 - accuracy: 0.9951 - val_loss: 0.2565 - val_accuracy: 0.9482\n",
      "val_f1: 0.93899, best_val_f1: 0.93899\n",
      "\n",
      "Epoch 00011: val_f1 improved from 0.93429 to 0.93899, saving model to model/cnn_1.h5\n",
      "Epoch 12/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0137 - accuracy: 0.9958 - val_loss: 0.2985 - val_accuracy: 0.9442\n",
      "val_f1: 0.93416, best_val_f1: 0.93899\n",
      "\n",
      "Epoch 00012: val_f1 did not improve from 0.93899\n",
      "Epoch 13/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0128 - accuracy: 0.9961 - val_loss: 0.2728 - val_accuracy: 0.9467\n",
      "val_f1: 0.93893, best_val_f1: 0.93899\n",
      "\n",
      "Epoch 00013: val_f1 did not improve from 0.93899\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 14/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0113 - accuracy: 0.9966 - val_loss: 0.2550 - val_accuracy: 0.9494\n",
      "val_f1: 0.94175, best_val_f1: 0.94175\n",
      "\n",
      "Epoch 00014: val_f1 improved from 0.93899 to 0.94175, saving model to model/cnn_1.h5\n",
      "Epoch 15/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0108 - accuracy: 0.9967 - val_loss: 0.2620 - val_accuracy: 0.9494\n",
      "val_f1: 0.94199, best_val_f1: 0.94199\n",
      "\n",
      "Epoch 00015: val_f1 improved from 0.94175 to 0.94199, saving model to model/cnn_1.h5\n",
      "Epoch 16/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0102 - accuracy: 0.9969 - val_loss: 0.2588 - val_accuracy: 0.9495\n",
      "val_f1: 0.94183, best_val_f1: 0.94199\n",
      "\n",
      "Epoch 00016: val_f1 did not improve from 0.94199\n",
      "Epoch 17/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0096 - accuracy: 0.9972 - val_loss: 0.2605 - val_accuracy: 0.9493\n",
      "val_f1: 0.94195, best_val_f1: 0.94199\n",
      "\n",
      "Epoch 00017: val_f1 did not improve from 0.94199\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 18/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0089 - accuracy: 0.9975 - val_loss: 0.2640 - val_accuracy: 0.9505\n",
      "val_f1: 0.94263, best_val_f1: 0.94263\n",
      "\n",
      "Epoch 00018: val_f1 improved from 0.94199 to 0.94263, saving model to model/cnn_1.h5\n",
      "Epoch 19/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0088 - accuracy: 0.9974 - val_loss: 0.2645 - val_accuracy: 0.9511\n",
      "val_f1: 0.94322, best_val_f1: 0.94322\n",
      "\n",
      "Epoch 00019: val_f1 improved from 0.94263 to 0.94322, saving model to model/cnn_1.h5\n",
      "Epoch 20/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0083 - accuracy: 0.9976 - val_loss: 0.2709 - val_accuracy: 0.9500\n",
      "val_f1: 0.94237, best_val_f1: 0.94322\n",
      "\n",
      "Epoch 00020: val_f1 did not improve from 0.94322\n",
      "Epoch 21/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0081 - accuracy: 0.9977 - val_loss: 0.2724 - val_accuracy: 0.9495\n",
      "val_f1: 0.94205, best_val_f1: 0.94322\n",
      "\n",
      "Epoch 00021: val_f1 did not improve from 0.94322\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 22/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0081 - accuracy: 0.9977 - val_loss: 0.2680 - val_accuracy: 0.9505\n",
      "val_f1: 0.94294, best_val_f1: 0.94322\n",
      "\n",
      "Epoch 00022: val_f1 did not improve from 0.94322\n",
      "Epoch 23/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0076 - accuracy: 0.9977 - val_loss: 0.2693 - val_accuracy: 0.9503\n",
      "val_f1: 0.94237, best_val_f1: 0.94322\n",
      "\n",
      "Epoch 00023: val_f1 did not improve from 0.94322\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 24/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0074 - accuracy: 0.9980 - val_loss: 0.2686 - val_accuracy: 0.9505\n",
      "val_f1: 0.94273, best_val_f1: 0.94322\n",
      "\n",
      "Epoch 00024: val_f1 did not improve from 0.94322\n",
      "Epoch 00024: early stopping\n",
      "Train on 160000 samples, validate on 40000 samples\n",
      "Epoch 1/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.7967 - accuracy: 0.7603 - val_loss: 0.3082 - val_accuracy: 0.9108\n",
      "val_f1: 0.88919, best_val_f1: 0.88919\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.88919, saving model to model/cnn_2.h5\n",
      "Epoch 2/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.2206 - accuracy: 0.9314 - val_loss: 0.2810 - val_accuracy: 0.9175\n",
      "val_f1: 0.90351, best_val_f1: 0.90351\n",
      "\n",
      "Epoch 00002: val_f1 improved from 0.88919 to 0.90351, saving model to model/cnn_2.h5\n",
      "Epoch 3/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.1574 - accuracy: 0.9501 - val_loss: 0.2381 - val_accuracy: 0.9300\n",
      "val_f1: 0.92286, best_val_f1: 0.92286\n",
      "\n",
      "Epoch 00003: val_f1 improved from 0.90351 to 0.92286, saving model to model/cnn_2.h5\n",
      "Epoch 4/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.1158 - accuracy: 0.9619 - val_loss: 0.3454 - val_accuracy: 0.9142\n",
      "val_f1: 0.88570, best_val_f1: 0.92286\n",
      "\n",
      "Epoch 00004: val_f1 did not improve from 0.92286\n",
      "Epoch 5/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0872 - accuracy: 0.9712 - val_loss: 0.3156 - val_accuracy: 0.9205\n",
      "val_f1: 0.90128, best_val_f1: 0.92286\n",
      "\n",
      "Epoch 00005: val_f1 did not improve from 0.92286\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 6/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0515 - accuracy: 0.9827 - val_loss: 0.2347 - val_accuracy: 0.9413\n",
      "val_f1: 0.93221, best_val_f1: 0.93221\n",
      "\n",
      "Epoch 00006: val_f1 improved from 0.92286 to 0.93221, saving model to model/cnn_2.h5\n",
      "Epoch 7/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0357 - accuracy: 0.9880 - val_loss: 0.3070 - val_accuracy: 0.9341\n",
      "val_f1: 0.92152, best_val_f1: 0.93221\n",
      "\n",
      "Epoch 00007: val_f1 did not improve from 0.93221\n",
      "Epoch 8/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0298 - accuracy: 0.9901 - val_loss: 0.3260 - val_accuracy: 0.9323\n",
      "val_f1: 0.92110, best_val_f1: 0.93221\n",
      "\n",
      "Epoch 00008: val_f1 did not improve from 0.93221\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 9/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0214 - accuracy: 0.9934 - val_loss: 0.3449 - val_accuracy: 0.9303\n",
      "val_f1: 0.92378, best_val_f1: 0.93221\n",
      "\n",
      "Epoch 00009: val_f1 did not improve from 0.93221\n",
      "Epoch 10/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0171 - accuracy: 0.9946 - val_loss: 0.2844 - val_accuracy: 0.9418\n",
      "val_f1: 0.93675, best_val_f1: 0.93675\n",
      "\n",
      "Epoch 00010: val_f1 improved from 0.93221 to 0.93675, saving model to model/cnn_2.h5\n",
      "Epoch 11/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0147 - accuracy: 0.9953 - val_loss: 0.3145 - val_accuracy: 0.9402\n",
      "val_f1: 0.93467, best_val_f1: 0.93675\n",
      "\n",
      "Epoch 00011: val_f1 did not improve from 0.93675\n",
      "Epoch 12/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0135 - accuracy: 0.9957 - val_loss: 0.2687 - val_accuracy: 0.9474\n",
      "val_f1: 0.94170, best_val_f1: 0.94170\n",
      "\n",
      "Epoch 00012: val_f1 improved from 0.93675 to 0.94170, saving model to model/cnn_2.h5\n",
      "Epoch 13/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0122 - accuracy: 0.9961 - val_loss: 0.3160 - val_accuracy: 0.9413\n",
      "val_f1: 0.93739, best_val_f1: 0.94170\n",
      "\n",
      "Epoch 00013: val_f1 did not improve from 0.94170\n",
      "Epoch 14/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0114 - accuracy: 0.9963 - val_loss: 0.3195 - val_accuracy: 0.9427\n",
      "val_f1: 0.93670, best_val_f1: 0.94170\n",
      "\n",
      "Epoch 00014: val_f1 did not improve from 0.94170\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 15/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0093 - accuracy: 0.9970 - val_loss: 0.3008 - val_accuracy: 0.9478\n",
      "val_f1: 0.94218, best_val_f1: 0.94218\n",
      "\n",
      "Epoch 00015: val_f1 improved from 0.94170 to 0.94218, saving model to model/cnn_2.h5\n",
      "Epoch 16/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0087 - accuracy: 0.9973 - val_loss: 0.2932 - val_accuracy: 0.9485\n",
      "val_f1: 0.94240, best_val_f1: 0.94240\n",
      "\n",
      "Epoch 00016: val_f1 improved from 0.94218 to 0.94240, saving model to model/cnn_2.h5\n",
      "Epoch 17/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0075 - accuracy: 0.9977 - val_loss: 0.3045 - val_accuracy: 0.9478\n",
      "val_f1: 0.94288, best_val_f1: 0.94288\n",
      "\n",
      "Epoch 00017: val_f1 improved from 0.94240 to 0.94288, saving model to model/cnn_2.h5\n",
      "Epoch 18/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0076 - accuracy: 0.9975 - val_loss: 0.3524 - val_accuracy: 0.9418\n",
      "val_f1: 0.93855, best_val_f1: 0.94288\n",
      "\n",
      "Epoch 00018: val_f1 did not improve from 0.94288\n",
      "Epoch 19/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0072 - accuracy: 0.9979 - val_loss: 0.3152 - val_accuracy: 0.9477\n",
      "val_f1: 0.94202, best_val_f1: 0.94288\n",
      "\n",
      "Epoch 00019: val_f1 did not improve from 0.94288\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 20/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0060 - accuracy: 0.9983 - val_loss: 0.3104 - val_accuracy: 0.9478\n",
      "val_f1: 0.94269, best_val_f1: 0.94288\n",
      "\n",
      "Epoch 00020: val_f1 did not improve from 0.94288\n",
      "Epoch 21/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0057 - accuracy: 0.9982 - val_loss: 0.3087 - val_accuracy: 0.9493\n",
      "val_f1: 0.94389, best_val_f1: 0.94389\n",
      "\n",
      "Epoch 00021: val_f1 improved from 0.94288 to 0.94389, saving model to model/cnn_2.h5\n",
      "Epoch 22/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0056 - accuracy: 0.9983 - val_loss: 0.3077 - val_accuracy: 0.9489\n",
      "val_f1: 0.94433, best_val_f1: 0.94433\n",
      "\n",
      "Epoch 00022: val_f1 improved from 0.94389 to 0.94433, saving model to model/cnn_2.h5\n",
      "Epoch 23/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0056 - accuracy: 0.9983 - val_loss: 0.3052 - val_accuracy: 0.9502\n",
      "val_f1: 0.94552, best_val_f1: 0.94552\n",
      "\n",
      "Epoch 00023: val_f1 improved from 0.94433 to 0.94552, saving model to model/cnn_2.h5\n",
      "Epoch 24/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0052 - accuracy: 0.9985 - val_loss: 0.3105 - val_accuracy: 0.9499\n",
      "val_f1: 0.94486, best_val_f1: 0.94552\n",
      "\n",
      "Epoch 00024: val_f1 did not improve from 0.94552\n",
      "Epoch 25/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0049 - accuracy: 0.9987 - val_loss: 0.3205 - val_accuracy: 0.9482\n",
      "val_f1: 0.94376, best_val_f1: 0.94552\n",
      "\n",
      "Epoch 00025: val_f1 did not improve from 0.94552\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 26/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0047 - accuracy: 0.9987 - val_loss: 0.3175 - val_accuracy: 0.9493\n",
      "val_f1: 0.94470, best_val_f1: 0.94552\n",
      "\n",
      "Epoch 00026: val_f1 did not improve from 0.94552\n",
      "Epoch 27/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0046 - accuracy: 0.9987 - val_loss: 0.3199 - val_accuracy: 0.9494\n",
      "val_f1: 0.94467, best_val_f1: 0.94552\n",
      "\n",
      "Epoch 00027: val_f1 did not improve from 0.94552\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 28/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0042 - accuracy: 0.9988 - val_loss: 0.3119 - val_accuracy: 0.9504\n",
      "val_f1: 0.94550, best_val_f1: 0.94552\n",
      "\n",
      "Epoch 00028: val_f1 did not improve from 0.94552\n",
      "Epoch 00028: early stopping\n",
      "Train on 160000 samples, validate on 40000 samples\n",
      "Epoch 1/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.7800 - accuracy: 0.7668 - val_loss: 0.3051 - val_accuracy: 0.9089\n",
      "val_f1: 0.88385, best_val_f1: 0.88385\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.88385, saving model to model/cnn_3.h5\n",
      "Epoch 2/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.2203 - accuracy: 0.9312 - val_loss: 0.3676 - val_accuracy: 0.8935\n",
      "val_f1: 0.85718, best_val_f1: 0.88385\n",
      "\n",
      "Epoch 00002: val_f1 did not improve from 0.88385\n",
      "Epoch 3/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.1563 - accuracy: 0.9499 - val_loss: 0.3147 - val_accuracy: 0.9108\n",
      "val_f1: 0.89554, best_val_f1: 0.89554\n",
      "\n",
      "Epoch 00003: val_f1 improved from 0.88385 to 0.89554, saving model to model/cnn_3.h5\n",
      "Epoch 4/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.1163 - accuracy: 0.9622 - val_loss: 0.7369 - val_accuracy: 0.8213\n",
      "val_f1: 0.77309, best_val_f1: 0.89554\n",
      "\n",
      "Epoch 00004: val_f1 did not improve from 0.89554\n",
      "Epoch 5/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0865 - accuracy: 0.9708 - val_loss: 0.3700 - val_accuracy: 0.9020\n",
      "val_f1: 0.88499, best_val_f1: 0.89554\n",
      "\n",
      "Epoch 00005: val_f1 did not improve from 0.89554\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 6/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.0516 - accuracy: 0.9827 - val_loss: 0.2440 - val_accuracy: 0.9402\n",
      "val_f1: 0.91998, best_val_f1: 0.91998\n",
      "\n",
      "Epoch 00006: val_f1 improved from 0.89554 to 0.91998, saving model to model/cnn_3.h5\n",
      "Epoch 7/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.0366 - accuracy: 0.9880 - val_loss: 0.3925 - val_accuracy: 0.9130\n",
      "val_f1: 0.91037, best_val_f1: 0.91998\n",
      "\n",
      "Epoch 00007: val_f1 did not improve from 0.91998\n",
      "Epoch 8/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.0305 - accuracy: 0.9899 - val_loss: 0.2547 - val_accuracy: 0.9403\n",
      "val_f1: 0.92679, best_val_f1: 0.92679\n",
      "\n",
      "Epoch 00008: val_f1 improved from 0.91998 to 0.92679, saving model to model/cnn_3.h5\n",
      "Epoch 9/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.0257 - accuracy: 0.9913 - val_loss: 0.3230 - val_accuracy: 0.9339\n",
      "val_f1: 0.92617, best_val_f1: 0.92679\n",
      "\n",
      "Epoch 00009: val_f1 did not improve from 0.92679\n",
      "Epoch 10/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.0226 - accuracy: 0.9923 - val_loss: 0.3074 - val_accuracy: 0.9347\n",
      "val_f1: 0.92600, best_val_f1: 0.92679\n",
      "\n",
      "Epoch 00010: val_f1 did not improve from 0.92679\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 11/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.0175 - accuracy: 0.9941 - val_loss: 0.2755 - val_accuracy: 0.9460\n",
      "val_f1: 0.93253, best_val_f1: 0.93253\n",
      "\n",
      "Epoch 00011: val_f1 improved from 0.92679 to 0.93253, saving model to model/cnn_3.h5\n",
      "Epoch 12/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.0130 - accuracy: 0.9958 - val_loss: 0.3252 - val_accuracy: 0.9391\n",
      "val_f1: 0.92967, best_val_f1: 0.93253\n",
      "\n",
      "Epoch 00012: val_f1 did not improve from 0.93253\n",
      "Epoch 13/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.0122 - accuracy: 0.9960 - val_loss: 0.3269 - val_accuracy: 0.9409\n",
      "val_f1: 0.93079, best_val_f1: 0.93253\n",
      "\n",
      "Epoch 00013: val_f1 did not improve from 0.93253\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 14/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.0102 - accuracy: 0.9968 - val_loss: 0.2842 - val_accuracy: 0.9474\n",
      "val_f1: 0.93727, best_val_f1: 0.93727\n",
      "\n",
      "Epoch 00014: val_f1 improved from 0.93253 to 0.93727, saving model to model/cnn_3.h5\n",
      "Epoch 15/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0086 - accuracy: 0.9973 - val_loss: 0.3189 - val_accuracy: 0.9443\n",
      "val_f1: 0.93447, best_val_f1: 0.93727\n",
      "\n",
      "Epoch 00015: val_f1 did not improve from 0.93727\n",
      "Epoch 16/30\n",
      "160000/160000 [==============================] - 336s 2ms/step - loss: 0.0079 - accuracy: 0.9976 - val_loss: 0.2942 - val_accuracy: 0.9467\n",
      "val_f1: 0.93650, best_val_f1: 0.93727\n",
      "\n",
      "Epoch 00016: val_f1 did not improve from 0.93727\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 17/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.0074 - accuracy: 0.9978 - val_loss: 0.2931 - val_accuracy: 0.9479\n",
      "val_f1: 0.93926, best_val_f1: 0.93926\n",
      "\n",
      "Epoch 00017: val_f1 improved from 0.93727 to 0.93926, saving model to model/cnn_3.h5\n",
      "Epoch 18/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.0067 - accuracy: 0.9980 - val_loss: 0.2834 - val_accuracy: 0.9507\n",
      "val_f1: 0.94223, best_val_f1: 0.94223\n",
      "\n",
      "Epoch 00018: val_f1 improved from 0.93926 to 0.94223, saving model to model/cnn_3.h5\n",
      "Epoch 19/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.0064 - accuracy: 0.9980 - val_loss: 0.2898 - val_accuracy: 0.9491\n",
      "val_f1: 0.94074, best_val_f1: 0.94223\n",
      "\n",
      "Epoch 00019: val_f1 did not improve from 0.94223\n",
      "Epoch 20/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.0058 - accuracy: 0.9983 - val_loss: 0.3042 - val_accuracy: 0.9476\n",
      "val_f1: 0.93887, best_val_f1: 0.94223\n",
      "\n",
      "Epoch 00020: val_f1 did not improve from 0.94223\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 21/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.0059 - accuracy: 0.9982 - val_loss: 0.2887 - val_accuracy: 0.9503\n",
      "val_f1: 0.94119, best_val_f1: 0.94223\n",
      "\n",
      "Epoch 00021: val_f1 did not improve from 0.94223\n",
      "Epoch 22/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.0057 - accuracy: 0.9983 - val_loss: 0.2928 - val_accuracy: 0.9505\n",
      "val_f1: 0.94160, best_val_f1: 0.94223\n",
      "\n",
      "Epoch 00022: val_f1 did not improve from 0.94223\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 23/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.0052 - accuracy: 0.9984 - val_loss: 0.2876 - val_accuracy: 0.9509\n",
      "val_f1: 0.94238, best_val_f1: 0.94238\n",
      "\n",
      "Epoch 00023: val_f1 improved from 0.94223 to 0.94238, saving model to model/cnn_3.h5\n",
      "Epoch 24/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.0051 - accuracy: 0.9986 - val_loss: 0.2865 - val_accuracy: 0.9508\n",
      "val_f1: 0.94242, best_val_f1: 0.94242\n",
      "\n",
      "Epoch 00024: val_f1 improved from 0.94238 to 0.94242, saving model to model/cnn_3.h5\n",
      "Epoch 25/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.0053 - accuracy: 0.9984 - val_loss: 0.2891 - val_accuracy: 0.9510\n",
      "val_f1: 0.94229, best_val_f1: 0.94242\n",
      "\n",
      "Epoch 00025: val_f1 did not improve from 0.94242\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 26/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.0048 - accuracy: 0.9987 - val_loss: 0.2889 - val_accuracy: 0.9509\n",
      "val_f1: 0.94209, best_val_f1: 0.94242\n",
      "\n",
      "Epoch 00026: val_f1 did not improve from 0.94242\n",
      "Epoch 27/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.0049 - accuracy: 0.9986 - val_loss: 0.2907 - val_accuracy: 0.9509\n",
      "val_f1: 0.94243, best_val_f1: 0.94243\n",
      "\n",
      "Epoch 00027: val_f1 improved from 0.94242 to 0.94243, saving model to model/cnn_3.h5\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 28/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.0049 - accuracy: 0.9985 - val_loss: 0.2904 - val_accuracy: 0.9507\n",
      "val_f1: 0.94205, best_val_f1: 0.94243\n",
      "\n",
      "Epoch 00028: val_f1 did not improve from 0.94243\n",
      "Epoch 29/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.0047 - accuracy: 0.9987 - val_loss: 0.2901 - val_accuracy: 0.9510\n",
      "val_f1: 0.94245, best_val_f1: 0.94245\n",
      "\n",
      "Epoch 00029: val_f1 improved from 0.94243 to 0.94245, saving model to model/cnn_3.h5\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 30/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.0049 - accuracy: 0.9986 - val_loss: 0.2901 - val_accuracy: 0.9508\n",
      "val_f1: 0.94236, best_val_f1: 0.94245\n",
      "\n",
      "Epoch 00030: val_f1 did not improve from 0.94245\n",
      "Train on 160000 samples, validate on 40000 samples\n",
      "Epoch 1/30\n",
      "160000/160000 [==============================] - 338s 2ms/step - loss: 0.7758 - accuracy: 0.7682 - val_loss: 0.4283 - val_accuracy: 0.8598\n",
      "val_f1: 0.82887, best_val_f1: 0.82887\n",
      "\n",
      "Epoch 00001: val_f1 improved from -inf to 0.82887, saving model to model/cnn_4.h5\n",
      "Epoch 2/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.2216 - accuracy: 0.9305 - val_loss: 0.3803 - val_accuracy: 0.8863\n",
      "val_f1: 0.84619, best_val_f1: 0.84619\n",
      "\n",
      "Epoch 00002: val_f1 improved from 0.82887 to 0.84619, saving model to model/cnn_4.h5\n",
      "Epoch 3/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.1576 - accuracy: 0.9500 - val_loss: 0.3417 - val_accuracy: 0.9007\n",
      "val_f1: 0.88399, best_val_f1: 0.88399\n",
      "\n",
      "Epoch 00003: val_f1 improved from 0.84619 to 0.88399, saving model to model/cnn_4.h5\n",
      "Epoch 4/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.1150 - accuracy: 0.9627 - val_loss: 0.4673 - val_accuracy: 0.8824\n",
      "val_f1: 0.86494, best_val_f1: 0.88399\n",
      "\n",
      "Epoch 00004: val_f1 did not improve from 0.88399\n",
      "Epoch 5/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.0877 - accuracy: 0.9706 - val_loss: 0.3014 - val_accuracy: 0.9229\n",
      "val_f1: 0.90185, best_val_f1: 0.90185\n",
      "\n",
      "Epoch 00005: val_f1 improved from 0.88399 to 0.90185, saving model to model/cnn_4.h5\n",
      "Epoch 6/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.0660 - accuracy: 0.9779 - val_loss: 0.5777 - val_accuracy: 0.8777\n",
      "val_f1: 0.84491, best_val_f1: 0.90185\n",
      "\n",
      "Epoch 00006: val_f1 did not improve from 0.90185\n",
      "Epoch 7/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.0521 - accuracy: 0.9825 - val_loss: 0.3349 - val_accuracy: 0.9202\n",
      "val_f1: 0.89951, best_val_f1: 0.90185\n",
      "\n",
      "Epoch 00007: val_f1 did not improve from 0.90185\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 8/30\n",
      "160000/160000 [==============================] - 337s 2ms/step - loss: 0.0334 - accuracy: 0.9885 - val_loss: 0.2738 - val_accuracy: 0.9409\n",
      "val_f1: 0.93320, best_val_f1: 0.93320\n",
      "\n",
      "Epoch 00008: val_f1 improved from 0.90185 to 0.93320, saving model to model/cnn_4.h5\n",
      "Epoch 9/30\n",
      "100096/160000 [=================>............] - ETA: 1:58 - loss: 0.0223 - accuracy: 0.9927"
     ]
    }
   ],
   "source": [
    "bs = 256\n",
    "monitor = 'val_f1'\n",
    "filter_sizes = [3, 4, 5, 10]\n",
    "num_filters = 128\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "for fold_id, (train_index, val_index) in enumerate(kfold.split(all_index, df_data.iloc[all_index]['label'])):\n",
    "    train_x = seqs[train_index]\n",
    "    val_x = seqs[val_index]\n",
    "\n",
    "    label = df_data['label'].values\n",
    "    train_y = label[train_index]\n",
    "    val_y = label[val_index]\n",
    "    \n",
    "    model_path = 'model/cnn_{}.h5'.format(fold_id)\n",
    "    checkpoint = ModelCheckpoint(model_path, monitor=monitor, verbose=1, save_best_only=True, mode='max', save_weights_only=True)\n",
    "    earlystopping = EarlyStopping(monitor=monitor, patience=5, verbose=1, mode='max')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor=monitor, factor=0.5, patience=2, mode='max', verbose=1)\n",
    "    \n",
    "    model = build_model(embedding, seq_len, filter_sizes, num_filters)\n",
    "    model.fit(train_x, train_y, batch_size=bs, epochs=30,\n",
    "              validation_data=(val_x, val_y),\n",
    "              callbacks=[Evaluator(validation_data=(val_x, val_y)), checkpoint, reduce_lr, earlystopping], verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 20s 498us/step\n",
      "50000/50000 [==============================] - 24s 480us/step\n",
      "40000/40000 [==============================] - 20s 499us/step\n",
      "50000/50000 [==============================] - 24s 481us/step\n",
      "40000/40000 [==============================] - 20s 499us/step\n",
      "50000/50000 [==============================] - 24s 481us/step\n",
      "40000/40000 [==============================] - 20s 499us/step\n",
      "50000/50000 [==============================] - 24s 480us/step\n",
      "40000/40000 [==============================] - 20s 500us/step\n",
      "50000/50000 [==============================] - 24s 482us/step\n"
     ]
    }
   ],
   "source": [
    "oof_pred = np.zeros((len(all_index), 14))\n",
    "test_pred = np.zeros((len(test_index), 14))\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "for fold_id, (train_index, val_index) in enumerate(kfold.split(all_index, df_data.iloc[all_index]['label'])):\n",
    "    model = build_model(embedding, seq_len, filter_sizes, num_filters)\n",
    "    model_path = 'model/cnn_{}.h5'.format(fold_id)\n",
    "    model.load_weights(model_path)\n",
    "    \n",
    "    val_x = seqs[val_index]\n",
    "    prob = model.predict(val_x, batch_size=bs, verbose=1)\n",
    "    oof_pred[val_index] = prob\n",
    "    \n",
    "    test_x = seqs[test_index]\n",
    "    prob = model.predict(test_x, batch_size=bs, verbose=1)\n",
    "    test_pred += prob / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9436911692076092\n"
     ]
    }
   ],
   "source": [
    "df_oof = df_data.loc[all_index][['label']]\n",
    "df_oof['predict'] = np.argmax(oof_pred, axis=1)\n",
    "f1score = f1_score(df_oof['label'], df_oof['predict'], average='macro')\n",
    "print(f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9436911692076092"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('prob/sub_5fold_cnn_{}.npy'.format(f1score), test_pred)\n",
    "np.save('prob/oof_5fold_cnn_{}.npy'.format(f1score), oof_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['label'] = np.argmax(test_pred, axis=1)\n",
    "sub.to_csv('sub/cnn_{}.csv'.format(f1score), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lstm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_tensorflow_py3",
   "language": "python",
   "name": "conda_tensorflow_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
